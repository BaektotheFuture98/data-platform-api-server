from airflow.sdk import task, dag
from airflow.models import Variable
from airflow.exceptions import AirflowFailException
from datetime import datetime, timezone # ğŸ’¡ days_ago ëŒ€ì‹  datetimeê³¼ timezone ì‚¬ìš©
import json

# ==========================================
# 1. Helper Functions (ì„¤ì • ë° ìŠ¤í‚¤ë§ˆ ìƒì„±)
# ==========================================

def _make_avro_schema(project_name: str, fields: list) -> str:
    """Avro ìŠ¤í‚¤ë§ˆ JSON ë¬¸ìì—´ ìƒì„±"""
    avro_fields = []
    for field_name in fields:
        field_type = "int" if "date" in field_name or "id" in field_name else "string"
        avro_fields.append({"name": field_name, "type": ["null", field_type], "default": None})

    data_schema = {
        "type": "record",
        "name": project_name,
        "namespace": "com.pipeline.dynamic",
        "fields": avro_fields
    }
    return json.dumps(data_schema, ensure_ascii=False)

def _connect_config(param: dict, chunk_index: int) -> dict:
    """ì²­í¬(í…Œì´ë¸”) ë³„ ì»¤ë„¥í„° ì„¤ì • ìƒì„±"""
    schema_registry_url = Variable.get("SCHEMA_REGISTRY_URL")
    
    suffix = f"-{chunk_index}"
    topic_name = f"{param['project_name']}-topic{suffix}"
    target_table_name = f"{param['table']}_{chunk_index}" 
    connector_name = f"{param['project_name']}-SinkConnector{suffix}"

    return {
        "name": connector_name,
        "config": {
            "connector.class": "io.confluent.connect.jdbc.JdbcSinkConnector",
            "tasks.max": "1",
            "topics": topic_name,
            
            "connection.url": f"jdbc:mysql://{param['host']}/{param['database']}",
            "connection.user": param["user"],
            "connection.password": param["password"],
            
            "table.name.format": target_table_name,
            "auto.create": "true",
            "auto.evolve": "true",
            
            "key.converter": "org.apache.kafka.connect.storage.StringConverter",
            "value.converter": "io.confluent.connect.avro.AvroConverter",
            "value.converter.schema.registry.url": schema_registry_url,
            "value.converter.schemas.enable": "true",
            "errors.tolerance": "none"
        }
    }

# ==========================================
# 2. Core Tasks
# ==========================================

@task
def plan_job(**kwargs) -> dict:
    """
    [Planning Task] API conf íŒŒì‹±, ES Count ì¡°íšŒ ë° ì²­í¬ ê³„íš ìˆ˜ë¦½
    """
    dag_run = kwargs.get('dag_run')
    param = dag_run.conf if dag_run else {}

    if not param:
        raise AirflowFailException("No configuration received from API Trigger.")

    # ES ì—°ê²° ë° Count ì¡°íšŒ ë¡œì§ (ì´ì „ ì½”ë“œì™€ ë™ì¼)
    from airflow.providers.elasticsearch.hooks.elasticsearch import ElasticsearchPythonHook
    es_hosts = [host.strip() for host in Variable.get("ELASTICSEARCH_HOSTS").split(",")]
    es_hook = ElasticsearchPythonHook(es_hosts=es_hosts, conn_kwargs={"basic_auth": ("elastic", "elastic")})
    
    try:
        count_res = es_hook.get_conn().count(index=param["elasticsearch_index"], body=param["query"])
        total_count = count_res["count"]
    except Exception as e:
        raise AirflowFailException(f"Failed to query Elasticsearch: {e}")

    chunk_size = 100000
    num_chunks = (total_count // chunk_size) + (1 if total_count % chunk_size > 0 else 0)
    if num_chunks == 0: num_chunks = 1 

    print(f"âœ‚ï¸ Plan: Need {num_chunks} tables/topics for {total_count} records. Service: {param.get('service')}")

    return {
        "base_param": param,
        "chunks": list(range(num_chunks)),
        "chunk_size": chunk_size,
        "schema_str": _make_avro_schema(param["project_name"], param["fields"])
    }

@task.branch(task_id="service_router")
def router(plan_info: dict) -> str:
    """
    [Branch Task] service í•„ë“œ ê°’ì— ë”°ë¼ ì‹¤í–‰ ê²½ë¡œ ë¶„ê¸°
    """
    service = plan_info["base_param"].get("service", "mysql")
    
    if service == "mysql":
        return "mysql_start_flow"
    elif service == "elasticsearch":
        return "elasticsearch_start_flow"
    elif service == "excel":
        return "excel_export_start_flow"
    else:
        raise AirflowFailException(f"Unsupported service type: {service}")

@task(task_id="register_schema_mapped")
def register_schema_mapped(chunk_index: int, base_param: dict, schema_str: str):
    """[Mapped Task] ê° ì²­í¬(Topic)ë³„ë¡œ ë™ì¼í•œ ìŠ¤í‚¤ë§ˆë¥¼ ë“±ë¡"""
    from schema_registry.client import SchemaRegistryClient, schema
    schema_registry_url = Variable.get("SCHEMA_REGISTRY_URL")
    client = SchemaRegistryClient(url=schema_registry_url)
    topic_name = f"{base_param['project_name']}-topic-{chunk_index}"
    subject = f"{topic_name}-value"
    avro_schema = schema.AvroSchema(schema_str)
    schema_id = client.register(subject, avro_schema)
    print(f"âœ… Schema registered for subject: {subject} (ID: {schema_id})")
    return schema_id

@task(task_id="create_connector_mapped")
def create_connector_mapped(chunk_index: int, base_param: dict) -> str:
    """[Mapped Task] ê° ì²­í¬(Table)ë³„ë¡œ Sink Connector ìƒì„±"""
    from kafka_connect import KafkaConnect 
    client = KafkaConnect(Variable.get("CONNECT_BOOTSTRAP_SERVERS"))
    config = _connect_config(base_param, chunk_index)
    
    try:
        response = client.create_connector(config)
        if response.status_code >= 400 and response.status_code != 409:
             raise AirflowFailException(f"Connector creation failed: {response.text}")
    except Exception as e:
        raise AirflowFailException(f"Connector error: {e}")
        
    print(f"âœ… Connector {config['name']} created/verified.")
    return config['name']

@task(task_id="ingest_data_router")
def ingest_data_router(plan_info: dict):
    """[Single Task] ë°ì´í„°ë¥¼ ì½ì–´ì„œ ê±´ìˆ˜ì— ë”°ë¼ ì•Œë§ì€ í† í”½ìœ¼ë¡œ ë¼ìš°íŒ…í•˜ë©° ì „ì†¡"""
    # ... (Confluent Kafka Producer ë° ES Scroll ë¡œì§ êµ¬í˜„ - ìƒëµ) ...
    print(f"ğŸš€ Data ingestion completed for {len(plan_info['chunks'])} chunks.")
    return plan_info

@task(trigger_rule="all_done", task_id="delete_connectors_mapped")
def delete_connectors_mapped(connector_name: str):
    """[Mapped Task] ìƒì„±í–ˆë˜ ì»¤ë„¥í„° ì‚­ì œ"""
    from kafka_connect import KafkaConnect
    if not connector_name: return
    client = KafkaConnect(Variable.get("CONNECT_BOOTSTRAP_SERVERS"))
    response = client.delete_connector(connector_name)
    if 200 <= response.status_code < 300 or response.status_code == 404:
        print(f"ğŸ—‘ï¸ Connector {connector_name} deleted.")
    else:
        print(f"âš ï¸ Failed to delete connector {connector_name}: {response.text}")

# --- Service Specific Dummy Start Tasks ---

@task(task_id="mysql_start_flow")
def mysql_start_flow(plan_info: dict) -> dict:
    """MySQL Sink Flowì˜ ì‹œì‘ ì§€ì """
    print("â¡ï¸ Starting MySQL/Kafka Sink flow.")
    return plan_info

@task(task_id="elasticsearch_start_flow")
def elasticsearch_start_flow(plan_info: dict):
    """Elasticsearch ì¸ë±ì‹± Flow ì‹œì‘ ì§€ì """
    raise NotImplementedError("Elasticsearch indexing flow not implemented yet.")

@task(task_id="excel_export_start_flow")
def excel_export_start_flow(plan_info: dict):
    """Excel íŒŒì¼ Export Flow ì‹œì‘ ì§€ì """
    raise NotImplementedError("Excel export flow not implemented yet.")


# ==========================================
# 3. DAG Definition
# ==========================================

@dag(
    dag_id="es_to_dynamic_sink_pipeline",
    start_date=datetime(2025, 1, 1, tzinfo=timezone.utc), 
    schedule=None,
    catchup=False
)
def migration_pipeline():
    
    # 1. ê³„íš ìˆ˜ë¦½ ë° ì„œë¹„ìŠ¤ ë¼ìš°íŒ…
    plan = plan_job()
    router_task = router(plan)

    # 2. MySQL ê²½ë¡œ ì‹œì‘ (ëŒ€ìš©ëŸ‰ ë¶„í•  ë¡œì§)
    mysql_starter = mysql_start_flow(plan)

    # 2-1. ìŠ¤í‚¤ë§ˆ ë° ì»¤ë„¥í„° ì¸í”„ë¼ êµ¬ì¶• (Dynamic Mapping)
    schema_strs = register_schema_mapped.partial(
        base_param=plan["base_param"], 
        schema_str=plan["schema_str"]
    ).expand(chunk_index=plan["chunks"])

    connector_names = create_connector_mapped.partial(
        base_param=plan["base_param"]
    ).expand(chunk_index=plan["chunks"])

    # 2-2. ë°ì´í„° ì „ì†¡
    ingestion = ingest_data_router(plan_info=plan)

    # 2-3. ì»¤ë„¥í„° ì‚­ì œ (í´ë¦°ì—…)
    clean_up = delete_connectors_mapped.expand(connector_name=connector_names)

    # ì˜ì¡´ì„± ì—°ê²° 
    router_task >> mysql_starter
    mysql_starter >> [schema_strs, connector_names]
    connector_names >> ingestion
    ingestion >> clean_up
    
    # 3. ê¸°íƒ€ ê²½ë¡œ ì—°ê²°
    router_task >> [
        elasticsearch_start_flow(plan),
        excel_export_start_flow(plan)
    ]

migration_pipeline()